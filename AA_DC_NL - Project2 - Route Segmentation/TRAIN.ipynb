{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "\"Libraries\"\n",
    "#from helpers import *\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mplimg\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Convolution2D, MaxPooling2D, LeakyReLU\n",
    "from keras.optimizers import Adamax\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from theano import ifelse\n",
    "import random\n",
    "random.seed(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 50 # Size of the window to be fed to the Neural Network.\n",
    "patch_size = 16 # This is a size of the patch that will be classified.\n",
    "padding = int((window_size - patch_size)/2) #padding to be added to the image to extract windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def Import_images():\n",
    "    \n",
    "    \"\"\"import all necessary images\"\"\"\n",
    "    # Load the training set\n",
    "    root_dir = \"training/\"\n",
    "\n",
    "    image_dir = root_dir + \"images/\"\n",
    "    files = os.listdir(image_dir)\n",
    "    n = len(files)\n",
    "    print(\"Loading \" + str(n) + \" images\")\n",
    "    imgs = np.asarray([mplimg.imread(image_dir + files[i]) for i in range(n)])\n",
    "\n",
    "    gt_dir = root_dir + \"groundtruth/\"\n",
    "    print(\"Loading \" + str(n) + \" images\")\n",
    "    gt_imgs = np.asarray([mplimg.imread(gt_dir + files[i]) for i in range(n)])\n",
    "    \n",
    "    return imgs,gt_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     8
    ]
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from PIL import Image\n",
    "import os\n",
    "from helper import *\n",
    "from Pipeline import *\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\"\"\"Make a second set of images with rotation of 45 degrees to make the program learn about diagonal road\"\"\"\n",
    "\n",
    "def data_augmentation(): \n",
    "    \n",
    "    print('Training Starts ...')\n",
    "\n",
    "    # Load the training set\n",
    "    root_dir = \"training/\"\n",
    "\n",
    "    SampleNumberForEachRotation = 25\n",
    "\n",
    "    #ORIGINAL IMAGE SET\n",
    "    image_dir = root_dir + \"images/\"\n",
    "    files = os.listdir(image_dir)\n",
    "    n = len(files)\n",
    "    print(\"Loading \" + str(n) + \" images\")\n",
    "    imgs = np.asarray([load_image(image_dir + files[i]) for i in range(n)])\n",
    "\n",
    "    #ORIGINAL IMAGE SET\n",
    "    gt_dir = root_dir + \"groundtruth/\"\n",
    "    files = os.listdir(gt_dir)\n",
    "    print(\"Loading \" + str(n) + \" images\")\n",
    "    gt_imgs = np.asarray([load_image(gt_dir + files[i]) for i in range(n)])\n",
    "\n",
    "    #DATA AUGEMENTATION by Rotation \n",
    "    path = \"training/images\"\n",
    "    path2 = \"training/groundtruth\"\n",
    "    p = Pipeline(path, path2) \n",
    "    p.rotate(probability=1, max_left_rotation=20, max_right_rotation=20, negative_rotation = False)\n",
    "    p.sample(SampleNumberForEachRotation) #NUMBER OF NEW IMAGES CREATED \n",
    "\n",
    "    os.rmdir('training/images/augmentedData/0')\n",
    "    os.rmdir('training/groundtruth/augmentedData/0')\n",
    "\n",
    "    #IMPORT THE AUGEMENTED IMAGE SET\n",
    "    image_dir = root_dir + \"images/augmentedData/\"\n",
    "    files = os.listdir(image_dir)\n",
    "    n = SampleNumberForEachRotation\n",
    "    print(\"Loading \" + str(n) + \" images\")\n",
    "    imgs_aug = np.asarray([load_image(image_dir + files[i]) for i in range(n)])\n",
    "\n",
    "    #MERGE ORIGINAL AND MERGED DATA \n",
    "    imgs_merged = []\n",
    "    for i in range(imgs.shape[0]):\n",
    "        imgs_merged.append(imgs[i])\n",
    "    for i in range(imgs_aug.shape[0]):\n",
    "        imgs_merged.append(imgs_aug[i])\n",
    "\n",
    "    gt_dir = root_dir + \"groundtruth/augmentedData/\"\n",
    "    print(\"Loading \" + str(n) + \" images\")\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.image as mpimg\n",
    "\n",
    "    #CONVERT IMAGES NAME TO 101, 102, etc.  \n",
    "    for i in range (1, n+1):\n",
    "        image = Image.open(image_dir + 'images_' + str(i) +'.png')\n",
    "        image.save(image_dir + 'satImage_' + str(i+100) +'.png') #Have the same files names as the orginial \n",
    "        os.remove(image_dir + 'images_' + str(i) + '.png')\n",
    "\n",
    "    #CONVERT IMAGES BACK TO GRAY SINCE ROTATION GIVES US RGB BACK \n",
    "    for i in range (1, n+1):\n",
    "        image = Image.open(gt_dir + 'groundtruth_' + str(i) +'.png').convert('L')\n",
    "        image.save(gt_dir + 'satImage_' + str(i+100) +'.png') #Have the same files names as the orginial \n",
    "        os.remove(gt_dir + 'groundtruth_' + str(i) + '.png')\n",
    "\n",
    "\n",
    "    p2 = Pipeline(path, path2)\n",
    "    p2.rotate(probability=1, max_left_rotation=20, max_right_rotation=20, negative_rotation = True)\n",
    "    p2.sample(SampleNumberForEachRotation) #NUMBER OF NEW IMAGES CREATED \n",
    "\n",
    "    os.rmdir('training/images/augmentedData/0')\n",
    "    os.rmdir('training/groundtruth/augmentedData/0')\n",
    "\n",
    "    #IMPORT THE AUGEMENTED IMAGE SET\n",
    "    image_dir = root_dir + \"images/augmentedData/\"\n",
    "    files = os.listdir(image_dir)\n",
    "    n = SampleNumberForEachRotation\n",
    "    print(\"Loading \" + str(n) + \" images\")\n",
    "    imgs_aug_2 = np.asarray([load_image(image_dir + files[i]) for i in range(n)])\n",
    "\n",
    "    for i in range(imgs_aug.shape[0]):\n",
    "        imgs_merged.append(imgs_aug_2[i])\n",
    "\n",
    "    ##CONTAINS ALL THE AUGEMENTED DATA (satimages)    \n",
    "    imgs_merged = np.array(imgs_merged)\n",
    "\n",
    "    gt_dir = root_dir + \"groundtruth/augmentedData/\"\n",
    "    print(\"Loading \" + str(n) + \" images\")\n",
    "\n",
    "    #CONVERT IMAGES NAME TO 126, 127, ..., etc.  \n",
    "    for i in range (1, n+1):\n",
    "        image = Image.open(image_dir + 'images_' + str(i) +'.png')\n",
    "        image.save(image_dir + 'satImage_' + str(i+125) +'.png') #Have the same files names as the orginial \n",
    "        os.remove(image_dir + 'images_' + str(i) + '.png')\n",
    "\n",
    "    #CONVERT IMAGES BACK TO GRAY SINCE ROTATION GIVES US RGB BACK \n",
    "    for i in range (1, n+1):\n",
    "        image = Image.open(gt_dir + 'groundtruth_' + str(i) +'.png').convert('L')\n",
    "        #image = load_image(gt_dir + files[1])\n",
    "        #image.convert('L')\n",
    "        #image.convert('LA')\n",
    "        image.save(gt_dir + 'satImage_' + str(i+125) +'.png') #Have the same files names as the orginial \n",
    "        os.remove(gt_dir + 'groundtruth_' + str(i) + '.png')\n",
    "\n",
    "    ############################## PUTTING LEFT AND RIGHT ROTATION TOGETHER ###################################\n",
    "    files = os.listdir(gt_dir)    \n",
    "    gt_imgs_aug = np.asarray([load_image(gt_dir + files[i]) for i in range(n)])\n",
    "\n",
    "    #MERGE ORIGINAL AND MERGED DATA \n",
    "    merged_aug_gt = []\n",
    "    for i in range(gt_imgs.shape[0]):\n",
    "        merged_aug_gt.append(gt_imgs[i])\n",
    "    for i in range(imgs_aug.shape[0]):\n",
    "        merged_aug_gt.append(gt_imgs_aug[i])\n",
    "    merged_aug_gt = np.array(merged_aug_gt)\n",
    "\n",
    "    #save the data in the variable that we are using later \n",
    "    imgs = imgs_merged\n",
    "    imgs_gt = gt_imgs_aug\n",
    "\n",
    "    #REMOVES THE PREVIOUS DATA AUGMENTATION DATA SET \n",
    "    shutil.rmtree('training/images/augmentedData')\n",
    "    shutil.rmtree('training/groundtruth/augmentedData')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 150 images\n",
      "Loading 150 images\n",
      "(400, 400, 3)\n",
      "Training Starts ...\n",
      "Loading 150 images\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mpimg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-783b2570686f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgt_imgs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mImport_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata_augmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-a9d622220b28>\u001b[0m in \u001b[0;36mdata_augmentation\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loading \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" images\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mload_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m#ORIGINAL IMAGE SET\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-a9d622220b28>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loading \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" images\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mload_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m#ORIGINAL IMAGE SET\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\MA1\\ML\\Project2 - ML - AADCNL\\helper.py\u001b[0m in \u001b[0;36mload_image\u001b[1;34m(infilename)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Helper functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmpimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mpimg' is not defined"
     ]
    }
   ],
   "source": [
    "imgs,gt_imgs=Import_images()\n",
    "print(imgs[0].shape)\n",
    "data_augmentation()\n",
    "print(imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def Preprocessing(Satellite_images,GT_images,rect_size,patch_size):\n",
    "    \"\"\"Preprocess images for Learning\"\"\"\n",
    "    \n",
    "    \"\"\"Minibatch\"\"\"\n",
    "    group_size = 300\n",
    "\n",
    "    def minibatching(): # Minibatch creation technics.\n",
    "\n",
    "            # Generate one minibatch\n",
    "            Satellite_images_group = np.zeros((group_size, rect_size, rect_size, 3)) #j'aurais aussi pu utiliser np.empty\n",
    "            GT_images_group = np.zeros((group_size, 2))\n",
    "            for i in range(group_size):\n",
    "                # Select a random image\n",
    "                number_image=Satellite_images.shape[0]\n",
    "                index = np.random.choice(number_image) #https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.random.choice.html\n",
    "\n",
    "                image_selected=Satellite_images[index]\n",
    "                image_ground_selected = GT_images[index]\n",
    "                image_selected_shape = Satellite_images[index].shape\n",
    "                image_ground_selected_shape = GT_images[index].shape\n",
    "                \n",
    "                \n",
    "                # Sample a random window from the image\n",
    "                center = np.random.randint(rect_size//2, image_selected_shape[0] - rect_size//2, 2) #Return random integers from the “discrete uniform” distribution of the specified dtype in the “half-open” interval [low, high). #https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.randint.html\n",
    "                sub_image = image_selected[center[0]-rect_size//2:center[0]+rect_size//2,center[1]-rect_size//2:center[1]+rect_size//2,:]\n",
    "                # We selected a part of the selected image next to the center that we choose randomly.\n",
    "                image_window = image_ground_selected[center[0]-patch_size//2:center[0]+patch_size//2,center[1]-patch_size//2:center[1]+patch_size//2]\n",
    "                # We selected the same part of the image_selected with the random center but this time we take it in the groundthruth image.\n",
    "\n",
    "                # The label : if the mean of the pixel of the group is superior to the theresold, we thus consider that this is road and the label is one.\n",
    "                mean = np.array([np.mean(image_window)])\n",
    "                threshold = 0.35\n",
    "                label = (mean > threshold) * 1\n",
    "                # This is the true label that we obtain thanks to the groundthruth image\n",
    "                # We could have rotate the images rondomly there but we decide to do another way : we create new image wich are rotated\n",
    "\n",
    "                label = np_utils.to_categorical(label, 2) #road =1 and not_a_road=0 --> Converts a class vector (integers) to binary class matrix.\n",
    "                Satellite_images_group[i,:] = sub_image\n",
    "                GT_images_group[i] = label\n",
    "\n",
    "            return Satellite_images_group, GT_images_group\n",
    "\n",
    "    #dans dossier on a du code pour baisser le temps de computation et pour fair marcher keras et theanos ensemble\n",
    "    while(1):\n",
    "        x,y=minibatching()\n",
    "        yield(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def Learn(ws,Satellite_images,GT_images,rect_size,patch_size):\n",
    "        \"\"\"Process learning on data\"\"\"\n",
    "        in_shape=(ws,ws,3)\n",
    "    \n",
    "        def define_model(in_shape):\n",
    "            CNN_model = Sequential()\n",
    "            CNN_model.add(Convolution2D(64,5,5,border_mode='same',input_shape=in_shape))\n",
    "            CNN_model.add(LeakyReLU(alpha=0.05))\n",
    "            CNN_model.add(MaxPooling2D(pool_size=(2,2),border_mode='same'))\n",
    "            \n",
    "            CNN_model.add(Convolution2D(128,3,3, border_mode='same'))\n",
    "            CNN_model.add(LeakyReLU(alpha=0.05))\n",
    "            CNN_model.add(MaxPooling2D(pool_size=(2,2),border_mode='same'))\n",
    "            \n",
    "            CNN_model.add(Convolution2D(256,3,3,border_mode='same'))\n",
    "            CNN_model.add(LeakyReLU(alpha=0.05))\n",
    "            CNN_model.add(MaxPooling2D(pool_size=(2,2),border_mode='same'))\n",
    "            CNN_model.add(Dropout(0.25))\n",
    "            \n",
    "            CNN_model.add(Convolution2D(512,3,3,border_mode='same'))\n",
    "            CNN_model.add(LeakyReLU(alpha=0.05))\n",
    "            CNN_model.add(MaxPooling2D(pool_size=(2,2),border_mode='same'))\n",
    "\n",
    "\n",
    "            CNN_model.add(Flatten())\n",
    "            CNN_model.add(Dense(256,W_regularizer=l2(1e-6)))\n",
    "            CNN_model.add(LeakyReLU(alpha=0.01))\n",
    "            CNN_model.add(Dropout(0.4))\n",
    "            CNN_model.add(Dense(128,W_regularizer=l2(1e-6)))\n",
    "            CNN_model.add(LeakyReLU(alpha=0.01))\n",
    "            CNN_model.add(Dense(64,W_regularizer=l2(1e-6)))\n",
    "            CNN_model.add(LeakyReLU(alpha=0.01))\n",
    "            CNN_model.add(Dense(32,W_regularizer=l2(1e-6)))\n",
    "            CNN_model.add(LeakyReLU(alpha=0.01))\n",
    "            CNN_model.add(Dense(2,W_regularizer=l2(1e-6),activation='softmax'))\n",
    "            CNN_model.summary()\n",
    "            return CNN_model\n",
    "\n",
    "        def compile_model(CNN_model):\n",
    "            CNN_model.compile(loss='categorical_crossentropy',optimizer='Adamax',metrics=['accuracy'])\n",
    "\n",
    "        print(\"Creating model\")\n",
    "        model = define_model(in_shape)\n",
    "        print(\"Compiling model\")\n",
    "        compile_model(model)\n",
    "        print(\"Fitting model\")\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='acc', factor=0.2, patience=3, min_lr=0.001)\n",
    "        try:\n",
    "            model.fit_generator(Preprocessing(Satellite_images,GT_images,rect_size,patch_size), steps_per_epoch=100, epochs=150, verbose=1, callbacks=[reduce_lr])\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "        print(\"Saving model\")\n",
    "        model.save(\"models/my_model.h5\")\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def Test(model,ws,ps,padding):\n",
    "    \"\"\"Test the model with testing data\"\"\"\n",
    "    def load_test_images():\n",
    "        test_dir = \"test_set_images/\"\n",
    "        files = os.listdir(test_dir)\n",
    "        n = len(files)-1\n",
    "        print(\"Loading \" + str(n) + \" images\")\n",
    "        test_img=[]\n",
    "        for i in range(n):\n",
    "            cd=test_dir+\"test_\"+str(i+1)+\"/\"\n",
    "            im = mplimg.imread(cd + os.listdir(cd)[0])\n",
    "            test_img.append(im)\n",
    "        test_img = np.asarray(test_img)\n",
    "        return test_img\n",
    "    \n",
    "    Test = load_test_images()\n",
    "    print(Test.shape)\n",
    "    \n",
    "    def extract_patches(test_img):\n",
    "        Test_patches = np.empty((72200,ws,ws,3))\n",
    "        imgs_id=[]\n",
    "        \n",
    "        def img_crop(im, w, h, l):\n",
    "            list_patches = np.empty((1444,ws,ws,3))\n",
    "            img_id=[]\n",
    "            imgwidth = im.shape[0]\n",
    "            imgheight = im.shape[1]\n",
    "            for i in range(0,imgheight-2*padding,h):\n",
    "                for j in range(0,imgwidth-2*padding,w):\n",
    "                    im_patch = im[j:(j+w+2*padding), i:i+h+2*padding, :]\n",
    "                    list_patches[int((i/h)*38+(j/w)),...]=im_patch\n",
    "                    if (l<9):\n",
    "                        img_id.append(\"00\"+str(l+1)+\"_\"+str(i)+\"_\"+str(j))\n",
    "                    else: img_id.append(\"0\"+str(l+1)+\"_\"+str(i)+\"_\"+str(j))\n",
    "            return np.asarray(list_patches), img_id\n",
    "        \n",
    "        def pad_img(img,p):\n",
    "            image=np.pad(img,((p,p),(p,p),(0,0)),'edge')\n",
    "            return image\n",
    "        \n",
    "        for k in range(test_img.shape[0]):\n",
    "            image = test_img[k]\n",
    "            img = pad_img(image,int(padding))\n",
    "            img_patches, img_id=img_crop(img,ps,ps,k)\n",
    "            Test_patches[int(k*1444):int((k+1)*1444),...] = img_patches\n",
    "            imgs_id = np.append(imgs_id,img_id)\n",
    "        Test_patches = np.asarray(Test_patches)\n",
    "        print(\"Test_patches size\")\n",
    "        print(Test_patches.shape)\n",
    "        print(\"imgs_id length\")\n",
    "        print(len(imgs_id))\n",
    "        return Test_patches,imgs_id\n",
    "            \n",
    "    Test_patches,img_id = extract_patches(Test)\n",
    "    Z = model.predict(Test_patches, verbose=1)\n",
    "    \n",
    "    Z=(Z[:,0]>Z[:,1])*1\n",
    "    \n",
    "    return Z,img_id,Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def visualize_prediction(predictions, Test, path):\n",
    "        my_file=Path(path)\n",
    "        if not my_file.is_dir(): #check if directory already exists\n",
    "            print(my_file.is_dir())\n",
    "            os.makedirs(path) #creates directory if it does not exist\n",
    "        for k in range(Test.shape[0]):\n",
    "            gt_values = predictions[k*1444:(k+1)*1444]\n",
    "            gt_test = np.empty((38,38))\n",
    "            for i in range(38):\n",
    "                for j in range(38):\n",
    "                    gt_test[j,i] = gt_values[i*38+j]\n",
    "            gt_test=np.asarray(gt_test)\n",
    "            fig = plt.figure(figsize=(10,10))\n",
    "            plt.imshow(Test[k],extent=(0,608,0,608))\n",
    "            plt.imshow(gt_test,cmap='gray_r',alpha=0.4,extent=(0,608,0,608))\n",
    "            plt.show()\n",
    "            #Save image in folder\n",
    "            image_name = path + \"prediction_image_\" + str(k)\n",
    "            fig.savefig(image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def create_submission(predictions, ids, submission_filename):\n",
    "\n",
    "    my_file=Path(submission_filename)\n",
    "    if my_file.is_file():\n",
    "        os.remove(submission_filename)\n",
    "    with open(submission_filename, 'w') as f:\n",
    "        f.write('id,prediction\\n')\n",
    "        for k in range(len(ids)):\n",
    "            f.writelines(ids[k]+','+str(predictions[k])+'\\n')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs,gt_imgs=Import_images()\n",
    "print(imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Learn(window_size,imgs,gt_imgs,window_size,patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred,img_id, Test_imgs=Test(model,window_size,patch_size,padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_prediction(pred,Test_imgs,\"prediction_images/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(1-pred,img_id,\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
