{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the best submission of the team \"My little Poly\".\n",
    "The code runtime is quite long since many calculations are\n",
    "performed in the code. To predict one single set of lambda\n",
    "and gamma hyperparameters.\n",
    "NOTE: The plots where made on with Jupiter Notebook in order\n",
    "to access to the plot library\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from implementations import *\n",
    "from proj1_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the best submission of the team My little Poly. The code runtime is quite long since many calculations are performed in the code. To predict one single set of lambda and gamma hyperparameters.\n"
     ]
    }
   ],
   "source": [
    "print(\"This is the best submission of the team My little Poly. The code runtime is quite long since many calculations are performed in the code. To predict one single set of lambda and gamma hyperparameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the train data\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading the train data\")\n",
    "y, tX, ids = load_csv_data('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanning and Standardizing the trainning & testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanning the missing values and standardizing the data\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleanning the missing values and standardizing the data\")\n",
    "# Removing bothering data and centering\n",
    "standardized_tX, stddev_train, mean_train = standardize_original(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PCA_flag=0\n",
    "if(PCA_flag):\n",
    "    print(\"Applying PCA on training dataset\")\n",
    "    eig_val, eig_vec, j = PCA(standardized_tX, threshold=0.7)\n",
    "    standardized_tX = standardized_tX.dot(eig_vec)\n",
    "    print(j)\n",
    "#After PCA the remaining number of dimensions (j) should not be too small for proper polynomial basis contruction (threshold>0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building polynomial basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building polynomial basis\n",
      "Creating indices for subsets of degree 2\n",
      "Creating indices for subsets of degree 3\n",
      "Computing first degree\n",
      "Computing second degree WITH combinations\n",
      "Computing from degree 3 to 10 WITHOUT combinations...\n",
      "Computing third degree WITH combinations...\n",
      "Standardizing the matrix contacting the Polynome\n"
     ]
    }
   ],
   "source": [
    "print(\"Building polynomial basis\")\n",
    "mat_train = build_poly_basis(standardized_tX)\n",
    "\n",
    "print(\"Standardizing the matrix contacting the Polynome\")\n",
    "standardized_mat, stddev_poly_train, mean_poly_train = standardize_basis(mat_train)\n",
    "tx = np.c_[np.ones(standardized_mat.shape[0]), standardized_mat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Begins Little Padawan Regression\n",
      "Please wait while the training completes\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Begins Little Padawan Regression\")\n",
    "print(\"Please wait while the training completes\")\n",
    "# Cross validating - FOR THE ENTIRE DATASET\n",
    "x1, x2, x3, x4, x5, y1, y2, y3, y4, y5 = cross_validation(y, tx)\n",
    "\n",
    "#Cross validating - FOR A THE SMALL DATA SET\n",
    "#x1, x2, x3, x4, x5, y1, y2, y3, y4, y5 = cross_validation_small(y, tx) #If you want to run the code on a smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.740000000000001e-09\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "# Best found parameters for reg_logistic_regression where:\n",
    "# lambda = 3.74*10**-9\n",
    "# gamma  = 10**5\n",
    "\n",
    "#LAST TESTED VALUES\n",
    "#lambdas_ = np.linspace(10**-9, 4*10**-9, 4)\n",
    "#gammas_ = np.linspace(4.5*10**5, 5.5*10**5, 3)\n",
    "\n",
    "# Best found parameters for reg_logistic_regression\n",
    "lambdas_ = [3.74*10**-9]\n",
    "gammas_ = [10**5]\n",
    "max_iters = 25\n",
    "initial_w = np.zeros((len(x1[0])))\n",
    "logistic_weights = []\n",
    "loss = []\n",
    "losses = np.zeros((len(gammas_), len(lambdas_)))\n",
    "\n",
    "for i in range(len(lambdas_)):\n",
    "    for j in range(len(gammas_)):\n",
    "        print(lambdas_[i])\n",
    "        print(gammas_[j])\n",
    "        reg_logistic_w1,loss1 = reg_logistic_regression(y1,x1,lambdas_[i],initial_w,max_iters,gammas_[j])\n",
    "        reg_logistic_w2,loss2 = reg_logistic_regression(y2,x2,lambdas_[i],initial_w,max_iters,gammas_[j])\n",
    "        reg_logistic_w3,loss3 = reg_logistic_regression(y3,x3,lambdas_[i],initial_w,max_iters,gammas_[j])\n",
    "        reg_logistic_w4,loss4 = reg_logistic_regression(y4,x4,lambdas_[i],initial_w,max_iters,gammas_[j])\n",
    "        reg_logistic_w5,loss5 = reg_logistic_regression(y5,x5,lambdas_[i],initial_w,max_iters,gammas_[j])\n",
    "        logistic_weights.append((reg_logistic_w1+reg_logistic_w2+reg_logistic_w3+reg_logistic_w4+reg_logistic_w5)/5)\n",
    "        losses[i,j] = ((loss1+loss2+loss3+loss4+loss5)/5)\n",
    "        loss.append((loss1+loss2+loss3+loss4+loss5)/5)\n",
    "\n",
    "min_losses = np.amin(losses)\n",
    "idx_min = np.argmin(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading the testing data\")\n",
    "_, testx, ids_test = load_csv_data('test.csv')\n",
    "\n",
    "print(\"Applying the same standardization to the testing set\")\n",
    "standardized_testx = standardize_test_original(testx, mean_train, stddev_train)\n",
    "\n",
    "if(PCA_flag):\n",
    "    print(\"Remove dimensions according to PCA\")\n",
    "    standardized_testx = standardized_testx.dot(eig_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building polynomial basis\")\n",
    "mat_test = build_poly_basis(standardized_testx)\n",
    "\n",
    "print(\"Standardizing again\")\n",
    "standardized_testmat = standardized_testx_basis(mat_test, mean_poly_train, stddev_poly_train)\n",
    "tX_test = np.c_[np.ones(standardized_testmat.shape[0]), standardized_testmat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the best hyperparamters Lambda and Gamma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_weights_min =  logistic_weights[idx_min]\n",
    "\n",
    "idx_lambdas = int(idx_min / len(lambdas_))\n",
    "idx_gammas = idx_min % len(lambdas_)\n",
    "\n",
    "print(\"The best submission parameter was [lambda]\", lambdas_[idx_lambdas])\n",
    "print(\"The best submission parameter was [gamma]\", gammas_[idx_gammas])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the final prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_labels(logistic_weights_min, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, 'predictions_ALL.csv')\n",
    "print(\"The prediction has been stored in the predictions.csv file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the prediction was accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Control the accuracy of the output\n",
    "y_pred = predict_labels(logistic_weights_min, tx)\n",
    "counter = 0.0\n",
    "for i in range (0, len(y)):\n",
    "    if (y_pred[i] == y[i]):\n",
    "        counter = counter + 1\n",
    "print ('Accuracy', counter /len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of the Results - Contour Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "# Make data.\n",
    "X = lambdas_\n",
    "Y = gammas_\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = losses\n",
    "\n",
    "# Plot the surface.\n",
    "surf = plt.contourf(X, Y, Z)\n",
    "# Customize the z axis.\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Gamma')\n",
    "plt.title('Loss Function for the hyperparameters $lambda$ and $gamma$')\n",
    "# Add a color bar which maps values to colors.\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "plt.savefig('Surface_plot_LR_jets.png', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
